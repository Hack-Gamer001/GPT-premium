# -*- coding: utf-8 -*-
"""python perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Lgr7ctWJMQkjBiHxOdFEsvOiGVzjkVcV
"""

import random
import math

class Perceptron:
    def __init__(self, num_features, activation='step', learning_rate=0.1, max_iterations=100):
        # Initialize weights randomly
        self.weights = [random.uniform(-1, 1) for _ in range(num_features)]
        self.bias = random.uniform(-1, 1)
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.activation = activation

    def activation_function(self, x):
        """
        Activation functions implementation with improved handling
        """
        try:
            if self.activation == 'linear':
                return x
            elif self.activation == 'step':
                return 1 if x > 0 else 0
            elif self.activation == 'sigmoid':
                return 1 / (1 + math.exp(-x))
            elif self.activation == 'tanh':
                return math.tanh(x)
            elif self.activation == 'relu':
                return max(0, x)
            elif self.activation == 'softmax':
                # Simplified softmax for binary classification
                return 1 / (1 + math.exp(-x))
            else:
                raise ValueError("Unsupported activation function")
        except (OverflowError, ValueError):
            # Handle extreme values
            return 1 if x > 0 else 0

    def predict(self, inputs):
        """
        Calculate weighted sum and apply activation function
        """
        # Calculate weighted sum
        weighted_sum = sum(w * x for w, x in zip(self.weights, inputs)) + self.bias

        # Apply activation function
        return self.activation_function(weighted_sum)

    def train(self, training_data, labels):
        """
        Train the perceptron using the given training data
        """
        for _ in range(self.max_iterations):
            total_error = 0
            for inputs, label in zip(training_data, labels):
                # Predict output
                prediction = self.predict(inputs)

                # Calculate error
                error = label - prediction
                total_error += abs(error)

                # Update weights
                for i in range(len(self.weights)):
                    self.weights[i] += self.learning_rate * error * inputs[i]

                # Update bias
                self.bias += self.learning_rate * error

            # Stop if error is minimal
            if total_error < 0.1:
                break

    def evaluate(self, test_data, test_labels):
        """
        Evaluate the perceptron's performance
        """
        correct_predictions = 0
        for inputs, label in zip(test_data, test_labels):
            # Use a threshold-based approach for prediction
            prediction = 1 if self.predict(inputs) >= 0.5 else 0
            if prediction == label:
                correct_predictions += 1

        return correct_predictions / len(test_labels)

# Example training data based on the spam classification table
def prepare_training_data():
    # Features: Total Words, Key Words Count, Spam Frequency
    training_data = [
        [7, 4, 0.57],  # Spam email example
        [8, 3, 0.38],  # Non-spam email example
        [7, 0, 0.0],   # Non-spam email example
        [6, 4, 0.67],  # Spam email example
        [5, 3, 0.6],   # Non-spam email example
        [7, 4, 0.57],  # Spam email example
    ]

    # Corresponding labels (1 for spam, 0 for non-spam)
    labels = [1, 0, 0, 1, 0, 1]

    return training_data, labels

def main():
    # Prepare training data
    training_data, labels = prepare_training_data()

    # Create perceptron with different activation functions
    activation_functions = ['step', 'sigmoid', 'tanh', 'relu', 'softmax', 'linear']

    for activation in activation_functions:
        print(f"\nTesting with {activation} activation:")
        perceptron = Perceptron(num_features=3, activation=activation)

        # Train the perceptron
        perceptron.train(training_data, labels)

        # Evaluate performance
        accuracy = perceptron.evaluate(training_data, labels)
        print(f"Accuracy: {accuracy * 100:.2f}%")

        # Test a new email
        test_email = [7, 4, 0.5]  # Example email features
        prediction = perceptron.predict(test_email)
        print(f"Prediction for test email: {'Spam' if prediction >= 0.5 else 'Not Spam'}")

if __name__ == "__main__":
    main()

import numpy as np
import matplotlib.pyplot as plt

class ActivationFunctions:
    @staticmethod
    def linear(x):
        """Linear activation function"""
        return x

    @staticmethod
    def step(x):
        """Step (Heaviside) activation function"""
        return 1 if x > 0 else 0

    @staticmethod
    def sigmoid(x):
        """Sigmoid activation function"""
        return 1 / (1 + np.exp(-x))

    @staticmethod
    def relu(x):
        """ReLU (Rectified Linear Unit) activation function"""
        return max(0, x)

    @staticmethod
    def softmax(x):
        """Softmax activation function"""
        exp_x = np.exp(x)
        return exp_x / np.sum(exp_x)

    @staticmethod
    def tanh(x):
        """Hyperbolic Tangent activation function"""
        return np.tanh(x)

class Perceptron:
    def __init__(self, input_size, activation_func='sigmoid', learning_rate=0.1, epochs=100):
        # Initialize weights and bias randomly
        self.weights = np.random.randn(input_size)
        self.bias = np.random.randn()
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.activation_func = activation_func

    def activate(self, x):
        """Apply selected activation function"""
        activation_map = {
            'linear': ActivationFunctions.linear,
            'step': ActivationFunctions.step,
            'sigmoid': ActivationFunctions.sigmoid,
            'relu': ActivationFunctions.relu,
            'softmax': ActivationFunctions.softmax,
            'tanh': ActivationFunctions.tanh
        }
        return activation_map[self.activation_func](x)

    def predict(self, inputs):
        """Make prediction"""
        # Calculate weighted sum
        z = np.dot(inputs, self.weights) + self.bias
        return self.activate(z)

    def train(self, X, y):
        """Train the perceptron"""
        for _ in range(self.epochs):
            for inputs, label in zip(X, y):
                # Predict
                prediction = self.predict(inputs)

                # Calculate error
                error = label - prediction

                # Update weights and bias
                self.weights += self.learning_rate * error * inputs
                self.bias += self.learning_rate * error

        return self.weights, self.bias

def visualize_activation_functions():
    """Visualize different activation functions"""
    plt.figure(figsize=(15, 10))

    # Range of input values
    x = np.linspace(-5, 5, 100)

    # Activation functions to plot
    funcs = {
        'Linear': ActivationFunctions.linear,
        'Step': ActivationFunctions.step,
        'Sigmoid': ActivationFunctions.sigmoid,
        'ReLU': ActivationFunctions.relu,
        'Softmax': lambda x: ActivationFunctions.softmax(np.array([x]))[0],
        'Tanh': ActivationFunctions.tanh
    }

    # Plot each activation function
    for i, (name, func) in enumerate(funcs.items(), 1):
        plt.subplot(2, 3, i)
        y = [func(val) for val in x]
        plt.plot(x, y)
        plt.title(f'{name} Activation Function')
        plt.xlabel('Input')
        plt.ylabel('Output')
        plt.grid(True)

    plt.tight_layout()
    plt.show()

def main():
    # Prepare training data from the spam classification example
    training_data = np.array([
        [7, 4, 0.57],  # Spam email example
        [8, 3, 0.38],  # Non-spam email example
        [7, 0, 0.0],   # Non-spam email example
        [6, 4, 0.67],  # Spam email example
        [5, 3, 0.6],   # Non-spam email example
        [7, 4, 0.57],  # Spam email example
    ])

    # Corresponding labels (1 for spam, 0 for non-spam)
    labels = np.array([1, 0, 0, 1, 0, 1])

    # Activation functions to test
    activation_functions = [
        'linear', 'step', 'sigmoid', 'relu', 'softmax', 'tanh'
    ]

    # Visualize activation functions
    visualize_activation_functions()

    # Train and evaluate perceptron with different activation functions
    plt.figure(figsize=(15, 10))

    for i, activation in enumerate(activation_functions, 1):
        # Create and train perceptron
        perceptron = Perceptron(input_size=3, activation_func=activation)
        weights, bias = perceptron.train(training_data, labels)

        # Evaluate performance
        predictions = [perceptron.predict(sample) for sample in training_data]
        accuracy = np.mean(np.round(predictions) == labels)

        # Plot results
        plt.subplot(2, 3, i)
        plt.scatter(training_data[:, 0], training_data[:, 1], c=labels, cmap='viridis')
        plt.title(f'{activation.capitalize()} Activation\nAccuracy: {accuracy:.2%}')
        plt.xlabel('Total Words')
        plt.ylabel('Key Words Count')



    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    main()

import numpy as np
import matplotlib.pyplot as plt

class Perceptron:
    def __init__(self, num_features, activation='sigmoid', learning_rate=0.1, threshold=0.5):
        # Initialize weights randomly
        self.weights = np.random.randn(num_features)
        self.bias = np.random.randn()
        self.learning_rate = learning_rate
        self.activation = activation
        self.threshold = threshold

    def activation_function(self, x):
        """
        Activation functions implementation
        """
        if self.activation == 'linear':
            return x
        elif self.activation == 'step':
            return 1 if x > 0 else 0
        elif self.activation == 'sigmoid':
            return 1 / (1 + np.exp(-x))
        elif self.activation == 'tanh':
            return np.tanh(x)
        elif self.activation == 'relu':
            return max(0, x)
        elif self.activation == 'softmax':
            # Simplified softmax for binary classification
            return 1 / (1 + np.exp(-x))
        else:
            raise ValueError("Unsupported activation function")

    def predict(self, inputs):
        """
        Calculate weighted sum and apply activation function
        """
        # Calculate weighted sum
        weighted_sum = np.dot(inputs, self.weights) + self.bias

        return self.activation_function(weighted_sum)

    def train(self, training_data, labels):
        """
        Train the perceptron and return detailed training information
        """
        training_details = []

        for epoch in range(10):  # Fixed number of iterations
            for inputs, label in zip(training_data, labels):
                # Predict output
                prediction = self.predict(inputs)

                # Calculate error
                error = label - prediction

                # Update weights
                for i in range(len(self.weights)):
                    self.weights[i] += self.learning_rate * error * inputs[i]

                # Update bias
                self.bias += self.learning_rate * error

                # Store training details
                training_details.append({
                    'epoch': epoch,
                    'inputs (x)': inputs,
                    'weights (w)': self.weights.copy(),
                    'bias': self.bias,
                    'weighted_sum': np.dot(inputs, self.weights) + self.bias,
                    'output (y)': prediction,
                    'expected_label': label
                })

        return training_details

    def evaluate(self, test_data, test_labels):
        """
        Evaluate the perceptron's performance
        """
        correct_predictions = 0
        evaluation_details = []

        for inputs, label in zip(test_data, test_labels):
            # Predict with threshold
            prediction = self.predict(inputs)
            classified_label = 1 if prediction >= self.threshold else 0

            # Check if prediction is correct
            if classified_label == label:
                correct_predictions += 1

            # Store evaluation details
            evaluation_details.append({
                'inputs (x)': inputs,
                'weights (w)': self.weights,
                'bias': self.bias,
                'output (y)': prediction,
                'classified_label': classified_label,
                'expected_label': label,
                'correct': classified_label == label
            })

        accuracy = correct_predictions / len(test_labels)
        return accuracy, evaluation_details

def main():
    # Training data based on spam classification
    training_data = np.array([
        [7, 4, 0.57],  # Spam email example
        [8, 3, 0.38],  # Non-spam email example
        [7, 0, 0.0],   # Non-spam email example
        [6, 4, 0.67],  # Spam email example
        [5, 3, 0.6],   # Non-spam email example
        [7, 4, 0.57],  # Spam email example
    ])

    # Corresponding labels (1 for spam, 0 for non-spam)
    labels = np.array([1, 0, 0, 1, 0, 1])

    # Activation functions to test
    activation_functions = ['step', 'sigmoid', 'tanh', 'relu', 'softmax', 'linear']

    # Visualization figure
    plt.figure(figsize=(20, 15))

    # Test each activation function
    for i, activation in enumerate(activation_functions, 1):
        print(f"\n{'='*50}")
        print(f"Testing with {activation.upper()} activation:")
        print(f"{'='*50}")

        # Create perceptron
        perceptron = Perceptron(num_features=3, activation=activation)

        # Train the perceptron and get training details
        training_details = perceptron.train(training_data, labels)

        # Evaluate performance
        accuracy, evaluation_details = perceptron.evaluate(training_data, labels)

        # Print detailed training information
        print("\nTraining Details:")
        for detail in training_details[-6:]:  # Last iteration for each sample
            print(f"Inputs (x): {detail['inputs (x)']}")
            print(f"Weights (w): {detail['weights (w)']}")
            print(f"Bias: {detail['bias']:.4f}")
            print(f"Weighted Sum: {detail['weighted_sum']:.4f}")
            print(f"Output (y): {detail['output (y)']:.4f}")
            print(f"Expected Label: {detail['expected_label']}")
            print("-" * 30)

        # Print evaluation details
        print("\nEvaluation Results:")
        print(f"Accuracy: {accuracy * 100:.2f}%")
        print("\nDetailed Evaluation:")
        for detail in evaluation_details:
            print(f"Inputs (x): {detail['inputs (x)']}")
            print(f"Output (y): {detail['output (y)']:.4f}")
            print(f"Classified Label: {detail['classified_label']}")
            print(f"Expected Label: {detail['expected_label']}")
            print(f"Correct: {detail['correct']}")
            print("-" * 30)

        # Visualization subplot
        plt.subplot(2, 3, i)
        plt.scatter(training_data[:, 0], training_data[:, 1], c=labels, cmap='viridis')
        plt.title(f'{activation.capitalize()} Activation\nAccuracy: {accuracy:.2%}')
        plt.xlabel('Total Words')
        plt.ylabel('Key Words Count')

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    main()

