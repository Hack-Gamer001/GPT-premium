{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# ============================\n",
        "# Vocabularios y mapeos básicos\n",
        "# ============================\n",
        "vocab_esp = ['yo', 'tú', 'él', 'ella', 'nosotros', 'ellos', 'como', 'bebo', 'veo', 'me gusta',\n",
        "             'soy', 'eres', 'es', 'el', 'un', 'una', 'manzana', 'agua', 'libro', 'casa']\n",
        "\n",
        "vocab_eng = ['i', 'you', 'he', 'she', 'we', 'they', 'eat', 'drink', 'see', 'like',\n",
        "             'am', 'are', 'is', 'the', 'a', 'an', 'apple', 'water', 'book', 'house']\n",
        "\n",
        "# Diccionarios para traducción directa\n",
        "eng2esp = dict(zip(vocab_eng, vocab_esp))\n",
        "esp2eng = dict(zip(vocab_esp, vocab_eng))\n",
        "\n",
        "# Mapeos para embeddings y decodificación\n",
        "esp2idx = {w:i for i,w in enumerate(vocab_esp)}\n",
        "eng2idx = {w:i for i,w in enumerate(vocab_eng)}\n",
        "idx2esp = {i:w for i,w in enumerate(vocab_esp)}\n",
        "idx2eng = {i:w for i,w in enumerate(vocab_eng)}\n",
        "\n",
        "# ============================\n",
        "# Utilidades\n",
        "# ============================\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
        "\n",
        "def positional_encoding(seq_len, d_model):\n",
        "    pos = np.arange(seq_len)[:, np.newaxis]\n",
        "    i = np.arange(d_model)[np.newaxis, :]\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    angle_rads = pos * angle_rates\n",
        "\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    return angle_rads\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    # Asegurarnos de que las dimensiones sean correctas para la transposición\n",
        "    # Q, K, V deben tener forma (batch_size, seq_len, d_model)\n",
        "    if len(Q.shape) == 3 and len(K.shape) == 3 and len(V.shape) == 3:\n",
        "        # Formato esperado para entrada de transformer: (batch, seq, dim)\n",
        "        matmul_qk = np.matmul(Q, K.transpose(0, 2, 1))\n",
        "    else:\n",
        "        # Adaptación para dimensiones menores o casos especiales\n",
        "        Q_reshaped = Q.reshape(1, *Q.shape) if len(Q.shape) < 3 else Q\n",
        "        K_reshaped = K.reshape(1, *K.shape) if len(K.shape) < 3 else K\n",
        "        V_reshaped = V.reshape(1, *V.shape) if len(V.shape) < 3 else V\n",
        "\n",
        "        # Asegurarnos de que tenga 3 dimensiones antes de transponer\n",
        "        if len(K_reshaped.shape) == 3:\n",
        "            matmul_qk = np.matmul(Q_reshaped, K_reshaped.transpose(0, 2, 1))\n",
        "        else:\n",
        "            # En último caso, última dimensión como profundidad\n",
        "            matmul_qk = np.matmul(Q_reshaped, K_reshaped.T)\n",
        "\n",
        "    dk = K.shape[-1]\n",
        "    scaled_logits = matmul_qk / np.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_logits += (mask * -1e9)\n",
        "\n",
        "    attention_weights = softmax(scaled_logits)\n",
        "    output = np.matmul(attention_weights, V)\n",
        "    return output, attention_weights\n",
        "\n",
        "def split_heads(x, num_heads):\n",
        "    batch_size, seq_len, d_model = x.shape\n",
        "    depth = d_model // num_heads\n",
        "    x = x.reshape(batch_size, seq_len, num_heads, depth)\n",
        "    x = x.transpose(0, 2, 1, 3)\n",
        "    return x\n",
        "\n",
        "def combine_heads(x):\n",
        "    batch_size, num_heads, seq_len, depth = x.shape\n",
        "    x = x.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, num_heads*depth)\n",
        "    return x\n",
        "\n",
        "def multi_head_attention(Q, K, V, num_heads):\n",
        "    batch_size = Q.shape[0] if len(Q.shape) >= 3 else 1\n",
        "    seq_len = Q.shape[1] if len(Q.shape) >= 3 else Q.shape[0]\n",
        "    d_model = Q.shape[-1]\n",
        "\n",
        "    # Asegurarnos de que las entradas tienen la forma correcta (batch_size, seq_len, d_model)\n",
        "    if len(Q.shape) < 3:\n",
        "        Q = Q.reshape(1, *Q.shape)\n",
        "    if len(K.shape) < 3:\n",
        "        K = K.reshape(1, *K.shape)\n",
        "    if len(V.shape) < 3:\n",
        "        V = V.reshape(1, *V.shape)\n",
        "\n",
        "    # Para casos con secuencias cortas, ajustar el número de cabezas\n",
        "    safe_num_heads = min(num_heads, d_model // 4)  # Evitar divisiones con cero o muy pequeñas\n",
        "    if safe_num_heads < 1:\n",
        "        safe_num_heads = 1\n",
        "\n",
        "    depth = d_model // safe_num_heads\n",
        "\n",
        "    # Inicializar pesos con una semilla para consistencia\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Para casos simples o palabras individuales, usamos atención simple\n",
        "    if seq_len == 1 or d_model < safe_num_heads * 2:\n",
        "        output, _ = scaled_dot_product_attention(Q, K, V)\n",
        "        return output\n",
        "\n",
        "    # Intentar realizar la atención multi-cabeza normal para entradas más complejas\n",
        "    try:\n",
        "        # División en cabezas\n",
        "        Q_split = []\n",
        "        K_split = []\n",
        "        V_split = []\n",
        "\n",
        "        for i in range(safe_num_heads):\n",
        "            start_idx = i * depth\n",
        "            end_idx = (i + 1) * depth\n",
        "            if end_idx > d_model:  # Evitar índices fuera de rango\n",
        "                end_idx = d_model\n",
        "\n",
        "            Q_split.append(Q[:, :, start_idx:end_idx])\n",
        "            K_split.append(K[:, :, start_idx:end_idx])\n",
        "            V_split.append(V[:, :, start_idx:end_idx])\n",
        "\n",
        "        # Procesar cada cabeza por separado\n",
        "        outputs = []\n",
        "        for i in range(safe_num_heads):\n",
        "            head_output, _ = scaled_dot_product_attention(Q_split[i], K_split[i], V_split[i])\n",
        "            outputs.append(head_output)\n",
        "\n",
        "        # Concatenar resultados si hay múltiples cabezas\n",
        "        if len(outputs) > 1:\n",
        "            concat = np.concatenate(outputs, axis=-1)\n",
        "        else:\n",
        "            concat = outputs[0]\n",
        "\n",
        "        return concat\n",
        "\n",
        "    except (ValueError, IndexError) as e:\n",
        "        # Si falla la atención multi-cabeza, volver a la atención simple\n",
        "        print(f\"Usando atención simple debido a: {e}\")\n",
        "        output, _ = scaled_dot_product_attention(Q, K, V)\n",
        "        return output\n",
        "\n",
        "def layer_norm(x, eps=1e-6):\n",
        "    mean = np.mean(x, axis=-1, keepdims=True)\n",
        "    std = np.std(x, axis=-1, keepdims=True)\n",
        "    return (x - mean) / (std + eps)\n",
        "\n",
        "def feed_forward(x, d_ff):\n",
        "    # Inicializar pesos con una semilla para reproducibilidad\n",
        "    np.random.seed(42)\n",
        "    W1 = np.random.randn(x.shape[-1], d_ff) * 0.1\n",
        "    b1 = np.zeros(d_ff)\n",
        "    W2 = np.random.randn(d_ff, x.shape[-1]) * 0.1\n",
        "    b2 = np.zeros(x.shape[-1])\n",
        "\n",
        "    x1 = np.maximum(0, np.dot(x, W1) + b1)\n",
        "    x2 = np.dot(x1, W2) + b2\n",
        "    return x2\n",
        "\n",
        "def encoder_block(x, num_heads, d_ff):\n",
        "    try:\n",
        "        attn_output = multi_head_attention(x, x, x, num_heads)\n",
        "        out1 = layer_norm(x + attn_output)\n",
        "        ff_output = feed_forward(out1, d_ff)\n",
        "        out2 = layer_norm(out1 + ff_output)\n",
        "        return out2\n",
        "    except Exception as e:\n",
        "        print(f\"Error en encoder_block: {e}\")\n",
        "        # Versión simplificada para casos problemáticos\n",
        "        return x\n",
        "\n",
        "def decoder_block(x, enc_output, num_heads, d_ff):\n",
        "    try:\n",
        "        masked_attn = multi_head_attention(x, x, x, num_heads)\n",
        "        out1 = layer_norm(x + masked_attn)\n",
        "        enc_dec_attn = multi_head_attention(out1, enc_output, enc_output, num_heads)\n",
        "        out2 = layer_norm(out1 + enc_dec_attn)\n",
        "        ff_output = feed_forward(out2, d_ff)\n",
        "        out3 = layer_norm(out2 + ff_output)\n",
        "        return out3\n",
        "    except Exception as e:\n",
        "        print(f\"Error en decoder_block: {e}\")\n",
        "        # Versión simplificada para casos problemáticos\n",
        "        return x\n",
        "\n",
        "# ============================\n",
        "# Funciones auxiliares para input/output\n",
        "# ============================\n",
        "\n",
        "def embed_sentence(sentence, vocab_dict, d_model):\n",
        "    tokens = sentence.lower().split()\n",
        "    embedded = np.zeros((len(tokens), d_model))\n",
        "    for i, word in enumerate(tokens):\n",
        "        idx = vocab_dict.get(word, 0)\n",
        "        one_hot = np.zeros(len(vocab_dict))\n",
        "        one_hot[idx] = 1\n",
        "        embedded[i, :len(vocab_dict)] = one_hot\n",
        "    return embedded[np.newaxis, :, :]\n",
        "\n",
        "def decode_output(output, idx2word):\n",
        "    tokens = []\n",
        "    for vec in output[0]:\n",
        "        idx = np.argmax(vec[:len(idx2word)])\n",
        "        tokens.append(idx2word.get(idx, '?'))\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# ============================\n",
        "# Función de traducción directa (palabra por palabra)\n",
        "# ============================\n",
        "\n",
        "def translate_simple(sentence):\n",
        "    words = sentence.lower().split()\n",
        "    translated = []\n",
        "    for word in words:\n",
        "        if word in eng2esp:\n",
        "            translated.append(eng2esp[word])\n",
        "        else:\n",
        "            translated.append(f\"[{word}]\")  # palabra desconocida\n",
        "    return ' '.join(translated)\n",
        "\n",
        "# ============================\n",
        "# Función principal traductor ing -> esp con modelo transformer\n",
        "# ============================\n",
        "\n",
        "def translate_eng2esp(sentence):\n",
        "    d_model = 32  # Aumentado para mejor capacidad del modelo\n",
        "    num_heads = 4\n",
        "    d_ff = 64\n",
        "\n",
        "    # Pre-procesamiento de la entrada\n",
        "    words = sentence.lower().split()\n",
        "\n",
        "    # Para evitar errores con entradas cortas\n",
        "    if len(words) == 1:\n",
        "        # Si solo hay una palabra, usar traducción directa\n",
        "        if words[0] in eng2esp:\n",
        "            print(\"Nota: Para una sola palabra, se usa traducción directa\")\n",
        "            return eng2esp[words[0]]\n",
        "\n",
        "    # Continuar con el modelo de transformer para frases más largas\n",
        "    try:\n",
        "        x = embed_sentence(sentence, eng2idx, d_model)\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        # Añadir codificación posicional\n",
        "        pos_encoding = positional_encoding(seq_len, d_model)\n",
        "        x_with_pos = x + pos_encoding[np.newaxis, :seq_len, :]\n",
        "\n",
        "        # Aplicar bloques de transformer\n",
        "        enc_out = encoder_block(x_with_pos, num_heads, d_ff)\n",
        "        dec_out = decoder_block(x_with_pos, enc_out, num_heads, d_ff)\n",
        "\n",
        "        # Decodificar la salida\n",
        "        translated = decode_output(dec_out, idx2esp)\n",
        "        return translated\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en el modelo transformer: {e}\")\n",
        "        print(\"Usando traducción de respaldo palabra por palabra...\")\n",
        "        return translate_simple(sentence)\n",
        "\n",
        "# ============================\n",
        "# Interfaz simple\n",
        "# ============================\n",
        "\n",
        "def run_simple_translator():\n",
        "    print(\"Traductor básico ING->ESP (palabra por palabra)\")\n",
        "    print(\"Vocabulario ING permitido:\", ', '.join(vocab_eng))\n",
        "\n",
        "    while True:\n",
        "        inp = input(\"\\nEscribe una frase en inglés (o 'salir'): \").strip().lower()\n",
        "        if inp == \"salir\":\n",
        "            break\n",
        "        output = translate_simple(inp)\n",
        "        print(\"Entrada:\", inp)\n",
        "        print(\"Traducción palabra por palabra:\", output)\n",
        "\n",
        "def run_transformer_translator():\n",
        "    print(\"Traductor ING->ESP basado en Transformer\")\n",
        "    print(\"Vocabulario ING permitido:\", ', '.join(vocab_eng))\n",
        "    print(\"Nota: Para mejores resultados, usa frases completas.\")\n",
        "\n",
        "    while True:\n",
        "        inp = input(\"\\nEscribe una frase en inglés (o 'salir'): \").strip().lower()\n",
        "        if inp == \"salir\":\n",
        "            break\n",
        "\n",
        "        words = inp.split()\n",
        "        unknown_words = [word for word in words if word not in eng2idx]\n",
        "\n",
        "        if unknown_words:\n",
        "            print(f\"❌ Palabras desconocidas: {', '.join(unknown_words)}\")\n",
        "            print(\"Solo se permiten palabras del vocabulario definido.\")\n",
        "        else:\n",
        "            print(\"Procesando...\")\n",
        "            try:\n",
        "                output = translate_eng2esp(inp)\n",
        "                print(\"Entrada:\", inp)\n",
        "                print(\"Traducción con transformer:\", output)\n",
        "\n",
        "                # Mostrar también la traducción palabra por palabra para comparar\n",
        "                simple = translate_simple(inp)\n",
        "                if output != simple:\n",
        "                    print(\"Traducción palabra por palabra:\", simple)\n",
        "            except Exception as e:\n",
        "                print(f\"Error al traducir: {str(e)}\")\n",
        "                print(\"Usando traducción simple como respaldo:\")\n",
        "                print(translate_simple(inp))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Selecciona el tipo de traductor:\")\n",
        "    print(\"1: Traductor básico (palabra por palabra)\")\n",
        "    print(\"2: Traductor avanzado (con modelo transformer)\")\n",
        "\n",
        "    choice = input(\"Ingresa tu opción (1 o 2): \")\n",
        "\n",
        "    if choice == \"1\":\n",
        "        run_simple_translator()\n",
        "    elif choice == \"2\":\n",
        "        run_transformer_translator()\n",
        "    else:\n",
        "        print(\"Opción inválida. Ejecutando traductor básico por defecto.\")\n",
        "        run_simple_translator()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kZedCWxXjnb",
        "outputId": "4f452fc9-e32f-413b-f43a-c770d1aa83b6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selecciona el tipo de traductor:\n",
            "1: Traductor básico (palabra por palabra)\n",
            "2: Traductor avanzado (con modelo transformer)\n",
            "Ingresa tu opción (1 o 2): 2\n",
            "Traductor ING->ESP basado en Transformer\n",
            "Vocabulario ING permitido: i, you, he, she, we, they, eat, drink, see, like, am, are, is, the, a, an, apple, water, book, house\n",
            "Nota: Para mejores resultados, usa frases completas.\n",
            "\n",
            "Escribe una frase en inglés (o 'salir'): you\n",
            "Procesando...\n",
            "Nota: Para una sola palabra, se usa traducción directa\n",
            "Entrada: you\n",
            "Traducción con transformer: tú\n",
            "\n",
            "Escribe una frase en inglés (o 'salir'): 1\n",
            "❌ Palabras desconocidas: 1\n",
            "Solo se permiten palabras del vocabulario definido.\n",
            "\n",
            "Escribe una frase en inglés (o 'salir'): salir\n"
          ]
        }
      ]
    }
  ]
}